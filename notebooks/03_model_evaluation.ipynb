{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "evaluation-header",
            "metadata": {},
            "source": [
                "# Phase 4: Rigorous Model Evaluation\n",
                "\n",
                "## 1. Objective\n",
                "Transitioning from model selection to deep validation. We focus on:\n",
                "1. **Imbalance-Aware Metrics**: Balanced Accuracy, Macro F1, ROC-AUC.\n",
                "2. **Error Patterns**: Manual inspection of misclassifications.\n",
                "3. **Threshold Optimization**: Tuning the classifier for critical minority classes (Negatives)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-eval",
            "metadata": {},
            "source": [
                "## 2. Setup and Data Loading\n",
                "Re-importing necessary components to ensure portability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports-eval",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import (\n",
                "    classification_report, \n",
                "    confusion_matrix, \n",
                "    balanced_accuracy_score, \n",
                "    roc_auc_score, \n",
                "    f1_score\n",
                ")\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.decomposition import TruncatedSVD\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.pipeline import Pipeline as ImbPipeline\n",
                "\n",
                "# Load cleaned data\n",
                "data_path = os.path.join('..', 'data', 'interim', 'cleaned_amazon.csv')\n",
                "df = pd.read_csv(data_path)\n",
                "\n",
                "# Target Binning\n",
                "df['sentiment'] = df['reviews.rating'].map({1: 0, 2: 0, 3: 1, 4: 2, 5: 2})\n",
                "df = df.dropna(subset=['cleaned_text', 'brand', 'categories'])\n",
                "df = df[df['cleaned_text'].str.strip().astype(bool)]\n",
                "\n",
                "# Split\n",
                "X = df[['cleaned_text', 'brand', 'categories']]\n",
                "y = df['sentiment']\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Data loaded. Test set size: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model-reconstruction",
            "metadata": {},
            "source": [
                "## 3. Model Reconstruction (Best Performer)\n",
                "From Phase 3, LogReg with SMOTE showed the best balance. We re-fit it here for evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fit-final-model",
            "metadata": {},
            "outputs": [],
            "source": [
                "text_transformer = Pipeline([\n",
                "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english', min_df=5)),\n",
                "    ('svd', TruncatedSVD(n_components=100, random_state=42)),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "preprocessor = ColumnTransformer([\n",
                "    ('text', text_transformer, 'cleaned_text'),\n",
                "    ('cat', OneHotEncoder(handle_unknown='ignore'), ['brand', 'categories'])\n",
                "])\n",
                "\n",
                "best_pipe = ImbPipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('smote', SMOTE(random_state=42)),\n",
                "    ('clf', LogisticRegression(max_iter=2000, n_jobs=-1, random_state=42))\n",
                "])\n",
                "\n",
                "print(\"Fitting final model for evaluation...\")\n",
                "best_pipe.fit(X_train, y_train)\n",
                "y_pred = best_pipe.predict(X_test)\n",
                "y_proba = best_pipe.predict_proba(X_test)\n",
                "print(\"Model ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metric-suite",
            "metadata": {},
            "source": [
                "## 4. Advanced Metric Suite\n",
                "Beyond Accuracy: Looking at the macro and balanced performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "calc-metrics",
            "metadata": {},
            "outputs": [],
            "source": [
                "bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
                "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
                "roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr')\n",
                "\n",
                "print(f\"Balanced Accuracy: {bal_acc:.4f}\")\n",
                "print(f\"F1 Score (Macro):   {f1_macro:.4f}\")\n",
                "print(f\"ROC-AUC (OvR):     {roc_auc:.4f}\")\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "visual-validation",
            "metadata": {},
            "source": [
                "## 5. Visual Validation: Confusion Matrix\n",
                "Where is the model getting confused?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot-cm",
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = confusion_matrix(y_test, y_pred)\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=['Neg', 'Neu', 'Pos'], \n",
                "            yticklabels=['Neg', 'Neu', 'Pos'])\n",
                "plt.title(\"Confusion Matrix Heatmap\")\n",
                "plt.xlabel(\"Predicted\")\n",
                "plt.ylabel(\"Actual\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "threshold-tuning",
            "metadata": {},
            "source": [
                "## 6. Threshold Tuning for Minority Class (Negatives)\n",
                "If detecting negative feedback is critical, we might lower the threshold for class 0."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "tuning-neg",
            "metadata": {},
            "outputs": [],
            "source": [
                "def custom_predict(proba, threshold=0.5, target_class=0):\n",
                "    \"\"\"\n",
                "    If proba[target_class] > threshold, return target_class.\n",
                "    Otherwise return the usual argmax.\n",
                "    \"\"\"\n",
                "    preds = []\n",
                "    for p in proba:\n",
                "        if p[target_class] >= threshold:\n",
                "            preds.append(target_class)\n",
                "        else:\n",
                "            preds.append(np.argmax(p))\n",
                "    return np.array(preds)\n",
                "\n",
                "# Experimenting with a 0.4 threshold for Negatives\n",
                "y_pred_tuned = custom_predict(y_proba, threshold=0.4, target_class=0)\n",
                "\n",
                "print(\"Tuned Classification Report (Threshold=0.4 for Negatives):\")\n",
                "print(classification_report(y_test, y_pred_tuned))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "error-analysis",
            "metadata": {},
            "source": [
                "## 7. Deep Error Analysis\n",
                "Inspecting samples where the model was highly confident but wrong."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "inspect-errors",
            "metadata": {},
            "outputs": [],
            "source": [
                "error_df = X_test.copy()\n",
                "error_df['actual'] = y_test\n",
                "error_df['pred'] = y_pred\n",
                "error_df['conf'] = y_proba.max(axis=1)\n",
                "\n",
                "# False Positives: Model thinks it's Positive (2), but it's Negative (0)\n",
                "false_positives = error_df[(error_df['actual'] == 0) & (error_df['pred'] == 2)].sort_values('conf', ascending=False)\n",
                "\n",
                "print(f\"Top 5 High-Confidence False Positives (Actual Neg -> Pred Pos):\")\n",
                "display(false_positives[['cleaned_text', 'conf']].head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "final-export",
            "metadata": {},
            "source": [
                "## 8. Conclusion\n",
                "Summary of findings:\n",
                "- Model struggles most with [Neutral Class / Sarcasm / etc].\n",
                "- Threshold tuning [Improved / Hurthe] recall for negatives.\n",
                "- Recommended next steps: [Data augmentation / More features / BERT?]"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}