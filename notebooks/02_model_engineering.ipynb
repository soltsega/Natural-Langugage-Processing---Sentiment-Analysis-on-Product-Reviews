{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 3: Advanced & Optimized Modeling\n",
                "\n",
                "## 1. Objective\n",
                "We aim to build a high-performance sentiment classifier using **Review Text**, **Brand**, and **Category**. \n",
                "\n",
                "### Performance Optimizations:\n",
                "1. **Parallelism**: Using `n_jobs=-1` for multi-core execution.\n",
                "2. **Dimensionality Reduction**: Using `TruncatedSVD` (Latent Semantic Analysis) to condense text features.\n",
                "3. **Fast Gradient Boosting**: Using XGBoost with `tree_method='hist'` for rapid training.\n",
                "4. **Reliability**: Unified Scikit-Learn Pipelines to prevent data leakage."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup and Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "import joblib\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.decomposition import TruncatedSVD\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "try:\n",
                "    import xgboost as xgb\n",
                "except ImportError:\n",
                "    print(\"XGBoost not found. Run: !pip install xgboost\")\n",
                "\n",
                "tqdm.pandas()\n",
                "\n",
                "# Load cleaned data\n",
                "data_path = os.path.join('..', 'data', 'interim', 'cleaned_amazon.csv')\n",
                "df = pd.read_csv(data_path)\n",
                "\n",
                "# Target Binning (Neg: 0, Neu: 1, Pos: 2)\n",
                "df['sentiment'] = df['reviews.rating'].map({1: 0, 2: 0, 3: 1, 4: 2, 5: 2})\n",
                "\n",
                "# Clean metadata and text\n",
                "df = df.dropna(subset=['cleaned_text', 'brand', 'categories'])\n",
                "df = df[df['cleaned_text'].str.strip().astype(bool)]\n",
                "\n",
                "print(f\"Final Dataset Shape: {df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Splitting (Stratified)\n",
                "We maintain an 80/20 split, ensuring sentiment ratios are preserved."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df[['cleaned_text', 'brand', 'categories']]\n",
                "y = df['sentiment']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Optimized Feature Pipeline\n",
                "We use `TruncatedSVD` to reduce dimensionality after TF-IDF. \n",
                "\n",
                "**Note**: Multinomial Naive Bayes cannot use SVD features because SVD produces negative values. We will define a specific pipeline for it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard preprocessor for most models\n",
                "text_transformer = Pipeline([\n",
                "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english', min_df=5)),\n",
                "    ('svd', TruncatedSVD(n_components=100, random_state=42))\n",
                "])\n",
                "\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('text', text_transformer, 'cleaned_text'),\n",
                "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['brand', 'categories'])\n",
                "    ],\n",
                "    remainder='drop'\n",
                ")\n",
                "\n",
                "# Raw TF-IDF preprocessor (specifically for MultinomialNB)\n",
                "nb_preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('text', TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english', min_df=5), 'cleaned_text'),\n",
                "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['brand', 'categories'])\n",
                "    ]\n",
                ")\n",
                "\n",
                "print(\"Preprocessors defined (Standard + NB-specific).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model 0: Multinomial Naive Bayes (Classic Baseline)\n",
                "Naive Bayes is a computationally cheap and effective baseline for text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "nb_pipeline = Pipeline([\n",
                "    ('preprocessor', nb_preprocessor),\n",
                "    ('clf', MultinomialNB())\n",
                "])\n",
                "\n",
                "print(\"Training Naive Bayes Baseline...\")\n",
                "nb_pipeline.fit(X_train, y_train)\n",
                "joblib.dump(nb_pipeline, '../models/nb_baseline_pipeline.pkl')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Optimized Model 1: Logistic Regression (+ Parallelism)\n",
                "We use `n_jobs=-1` to distribute training across all available CPUs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "lr_pipeline = Pipeline([\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('clf', LogisticRegression(class_weight='balanced', max_iter=2000, n_jobs=-1, random_state=42))\n",
                "])\n",
                "\n",
                "print(\"Training Optimized Logistic Regression...\")\n",
                "lr_pipeline.fit(X_train, y_train)\n",
                "joblib.dump(lr_pipeline, '../models/optimized_lr_pipeline.pkl')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Optimized Model 2: XGBoost (+ Hist method)\n",
                "Using `tree_method='hist'` drastically reduces training time on large datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "try:\n",
                "    xgb_pipeline = Pipeline([\n",
                "        ('preprocessor', preprocessor),\n",
                "        ('clf', xgb.XGBClassifier(\n",
                "            tree_method='hist', \n",
                "            n_jobs=-1, \n",
                "            random_state=42, \n",
                "            eval_metric='mlogloss'\n",
                "        ))\n",
                "    ])\n",
                "    \n",
                "    print(\"Training Fast XGBoost...\")\n",
                "    xgb_pipeline.fit(X_train, y_train)\n",
                "    joblib.dump(xgb_pipeline, '../models/optimized_xgb_pipeline.pkl')\n",
                "except NameError:\n",
                "    print(\"XGBoost missing.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Comparative Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate(pipeline, name):\n",
                "    y_pred = pipeline.predict(X_test)\n",
                "    print(f\"\\n--- {name} Report ---\")\n",
                "    print(classification_report(y_test, y_pred, target_names=['Neg', 'Neu', 'Pos']))\n",
                "    \n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    plt.figure(figsize=(6,4))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='inferno', xticklabels=['Neg','Neu','Pos'], yticklabels=['Neg','Neu','Pos'])\n",
                "    plt.title(f\"{name} Confusion Matrix\")\n",
                "    plt.show()\n",
                "\n",
                "evaluate(nb_pipeline, \"Naive Bayes Baseline\")\n",
                "evaluate(lr_pipeline, \"Optimized LogReg\")\n",
                "try: evaluate(xgb_pipeline, \"Optimized XGBoost\")\n",
                "except NameError: pass"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}